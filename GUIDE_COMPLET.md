# ğŸ‰ OpenRAG - Solution RAG complÃ¨te

## ğŸ“‹ RÃ©sumÃ© du projet

J'ai dÃ©veloppÃ© une solution RAG (Retrieval-Augmented Generation) complÃ¨te et prÃªte pour la production, conforme Ã  l'architecture dÃ©crite dans votre schÃ©ma.

## âœ… Ce qui a Ã©tÃ© crÃ©Ã©

### ğŸ—ï¸ Architecture microservices complÃ¨te

1. **API Gateway** (FastAPI) - Port 8000
   - Point d'entrÃ©e REST
   - Documentation Swagger automatique
   - Gestion des uploads et requÃªtes

2. **Orchestrateur** - Port 8001
   - Coordination du workflow RAG
   - Traitement asynchrone des documents
   - Gestion du pipeline d'ingestion

3. **Service d'Embeddings** - Port 8002
   - GÃ©nÃ©ration de vecteurs avec sentence-transformers
   - Support CPU/GPU
   - Traitement par batch optimisÃ©

4. **Infrastructure de stockage**
   - **MinIO** : Stockage d'objets S3 (ports 9000, 9001)
   - **Qdrant** : Base vectorielle pour la recherche sÃ©mantique (ports 6333, 6334)
   - **PostgreSQL** : MÃ©tadonnÃ©es et historique (port 5432)
   - **Redis** : Cache et files de tÃ¢ches (port 6379)

5. **LLM**
   - **Ollama** : Serveur LLM local (port 11434)
   - Support OpenAI et Anthropic Claude

### ğŸ“š Documentation Mintlify complÃ¨te

Documentation interactive professionnelle incluant :
- âœ… Introduction et guide de dÃ©marrage rapide
- âœ… Architecture dÃ©taillÃ©e avec diagrammes
- âœ… Guide d'installation pas Ã  pas
- âœ… RÃ©fÃ©rence API complÃ¨te
- âœ… Guides pratiques
- âœ… Documentation des prÃ©requis systÃ¨me

### ğŸ› ï¸ Scripts et outils

1. **setup.sh** : Installation automatique interactive
2. **test.sh** : Suite de tests automatisÃ©s
3. **Makefile** : Commandes utiles (make start, stop, logs, etc.)
4. **docker-compose.yml** : Configuration complÃ¨te des services

### ğŸ“„ Documentation projet

- README.md : Documentation principale
- QUICKSTART.md : Guide de dÃ©marrage rapide
- STRUCTURE.md : Structure dÃ©taillÃ©e du projet
- .env.example : Exemple de configuration

## ğŸš€ Comment dÃ©marrer

### Option 1 : Installation automatique (recommandÃ©)

```bash
cd /home/adminrag/openrag
make install
```

Cette commande va :
1. VÃ©rifier les prÃ©requis
2. CrÃ©er le fichier .env
3. Vous guider pour configurer le LLM
4. TÃ©lÃ©charger et dÃ©marrer tous les services
5. TÃ©lÃ©charger le modÃ¨le LLM (si Ollama)

### Option 2 : Installation manuelle

```bash
cd /home/adminrag/openrag

# 1. Configuration
cp .env.example .env
# Ã‰ditez .env selon vos besoins

# 2. DÃ©marrer les services
docker-compose up -d

# 3. TÃ©lÃ©charger le modÃ¨le Ollama (si utilisÃ©)
docker exec -it openrag-ollama ollama pull llama3.1:8b

# 4. VÃ©rifier le statut
make status
```

### Option 3 : Utiliser les scripts

```bash
cd /home/adminrag/openrag

# Installation interactive
bash scripts/setup.sh

# Tests
bash scripts/test.sh
```

## ğŸ“– Utilisation

### 1. Uploader un document

```bash
curl -X POST http://localhost:8000/documents/upload \
  -F "file=@votre_document.pdf" \
  -F "collection_id=default"
```

Formats supportÃ©s : PDF, DOCX, TXT, MD, etc.

### 2. Poser une question

```bash
curl -X POST http://localhost:8000/query \
  -H "Content-Type: application/json" \
  -d '{
    "query": "Quelle est la politique de remboursement ?",
    "max_results": 5,
    "use_llm": true
  }'
```

### 3. Lister les documents

```bash
curl http://localhost:8000/documents
```

### 4. AccÃ©der aux interfaces

- **API Swagger** : http://localhost:8000/docs
- **MinIO Console** : http://localhost:9001 (admin/admin123456)
- **Qdrant Dashboard** : http://localhost:6333/dashboard

## ğŸ¯ Commandes utiles

```bash
make start      # DÃ©marrer tous les services
make stop       # ArrÃªter tous les services
make restart    # RedÃ©marrer
make logs       # Voir les logs en temps rÃ©el
make logs-api   # Logs d'un service spÃ©cifique
make status     # Statut de tous les services
make test       # Lancer les tests
make backup     # Sauvegarder les donnÃ©es
make docs       # DÃ©marrer la documentation
make clean      # Nettoyer (âš ï¸ supprime les donnÃ©es)
```

## ğŸ”§ Configuration

### Fichier .env principal

```env
# LLM Configuration
LLM_PROVIDER=ollama          # ollama, openai, anthropic
LLM_MODEL=llama3.1:8b
# OPENAI_API_KEY=sk-...      # Si OpenAI
# ANTHROPIC_API_KEY=sk-ant-... # Si Anthropic

# Embedding Model
EMBEDDING_MODEL=sentence-transformers/all-MiniLM-L6-v2
EMBEDDING_DEVICE=cpu         # ou cuda pour GPU

# MinIO
MINIO_ROOT_USER=admin
MINIO_ROOT_PASSWORD=admin123456

# PostgreSQL
POSTGRES_USER=openrag
POSTGRES_PASSWORD=openrag123
POSTGRES_DB=openrag_db
```

### Choix du LLM

**Ollama (local, recommandÃ© pour commencer)**
```env
LLM_PROVIDER=ollama
LLM_MODEL=llama3.1:8b
```
ModÃ¨les suggÃ©rÃ©s : llama3.1:8b, phi3:mini, gemma:7b, mistral:7b

**OpenAI**
```env
LLM_PROVIDER=openai
LLM_MODEL=gpt-4-turbo
OPENAI_API_KEY=sk-...
```

**Anthropic Claude**
```env
LLM_PROVIDER=anthropic
LLM_MODEL=claude-3-sonnet-20240229
ANTHROPIC_API_KEY=sk-ant-...
```

## ğŸ“Š Monitoring (optionnel)

DÃ©marrer Prometheus et Grafana :

```bash
make monitoring-start
```

AccÃ¨s :
- Prometheus : http://localhost:9090
- Grafana : http://localhost:3000 (admin/admin)

## ğŸ­ Workflow RAG

### Ingestion de document

```
1. Upload â†’ MinIO (stockage fichier)
2. Extraction de texte (PDF, DOCX, etc.)
3. Chunking (dÃ©coupage en morceaux ~512 tokens)
4. GÃ©nÃ©ration d'embeddings (vectorisation)
5. Indexation dans Qdrant (base vectorielle)
6. MÃ©tadonnÃ©es dans PostgreSQL
```

### Traitement de requÃªte

```
1. Question utilisateur
2. Vectorisation de la requÃªte
3. Recherche des chunks similaires (Qdrant)
4. RÃ©cupÃ©ration des contenus (PostgreSQL)
5. Construction du contexte
6. GÃ©nÃ©ration de rÃ©ponse (LLM)
7. Retour rÃ©ponse + sources
```

## ğŸ“ Structure des fichiers crÃ©Ã©s

```
/home/adminrag/openrag/
â”œâ”€â”€ README.md
â”œâ”€â”€ QUICKSTART.md
â”œâ”€â”€ STRUCTURE.md
â”œâ”€â”€ Makefile
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ .env.example
â”œâ”€â”€ .gitignore
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â”œâ”€â”€ main.py
â”‚   â”‚   â””â”€â”€ Dockerfile
â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”œâ”€â”€ orchestrator/
â”‚   â”‚   â”‚   â”œâ”€â”€ main.py
â”‚   â”‚   â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ document_processor.py
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ vector_store.py
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ llm_service.py
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ storage.py
â”‚   â”‚   â”‚   â””â”€â”€ database/
â”‚   â”‚   â”‚       â””â”€â”€ db.py
â”‚   â”‚   â””â”€â”€ embedding/
â”‚   â”‚       â””â”€â”€ main.py
â”‚   â””â”€â”€ database/
â”‚       â””â”€â”€ init.sql
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ mint.json
â”‚   â”œâ”€â”€ introduction.mdx
â”‚   â”œâ”€â”€ quickstart.mdx
â”‚   â”œâ”€â”€ architecture.mdx
â”‚   â””â”€â”€ api-reference/
â”‚       â”œâ”€â”€ introduction.mdx
â”‚       â””â”€â”€ query/
â”‚           â””â”€â”€ process-query.mdx
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ setup.sh
â”‚   â””â”€â”€ test.sh
â””â”€â”€ monitoring/
    â””â”€â”€ prometheus.yml
```

## ğŸ“ Documentation

### Lancer la documentation Mintlify localement

```bash
cd /home/adminrag/openrag/docs
npx mintlify dev
```

Puis visitez http://localhost:3000

La documentation complÃ¨te inclut :
- Guide de dÃ©marrage rapide
- Architecture dÃ©taillÃ©e
- Installation complÃ¨te
- RÃ©fÃ©rence API
- Guides pratiques

## ğŸ” Tests

Lancer la suite de tests automatisÃ©s :

```bash
make test
# ou
bash scripts/test.sh
```

Les tests vÃ©rifient :
- âœ… SantÃ© de tous les services
- âœ… Upload de documents
- âœ… Traitement des documents
- âœ… RequÃªtes et recherche
- âœ… GÃ©nÃ©ration LLM

## ğŸ†˜ Troubleshooting

### Les services ne dÃ©marrent pas
```bash
docker-compose logs -f
```

### VÃ©rifier qu'Ollama a bien le modÃ¨le
```bash
docker exec -it openrag-ollama ollama list
docker exec -it openrag-ollama ollama pull llama3.1:8b
```

### VÃ©rifier le statut des documents
```bash
curl http://localhost:8000/documents | jq
```

### RedÃ©marrer un service spÃ©cifique
```bash
docker-compose restart api
docker-compose restart orchestrator
```

## ğŸ” SÃ©curitÃ© (pour la production)

âš ï¸ Avant de dÃ©ployer en production :

1. Changez tous les mots de passe par dÃ©faut dans .env
2. Ajoutez l'authentification Ã  l'API
3. Utilisez HTTPS/TLS
4. Configurez un firewall
5. Activez les logs d'audit
6. Mettez en place des backups rÃ©guliers

## ğŸ“ˆ Prochaines Ã©tapes recommandÃ©es

1. **Tester l'installation**
   ```bash
   make test
   ```

2. **Uploader vos premiers documents**
   ```bash
   curl -X POST http://localhost:8000/documents/upload -F "file=@document.pdf"
   ```

3. **Explorer la documentation interactive**
   ```bash
   cd docs && npx mintlify dev
   ```

4. **Configurer le monitoring** (optionnel)
   ```bash
   make monitoring-start
   ```

5. **Personnaliser selon vos besoins**
   - Ajustez les paramÃ¨tres dans .env
   - Choisissez votre LLM prÃ©fÃ©rÃ©
   - Configurez les collections de documents

## ğŸ“ Support

Pour toute question ou problÃ¨me :
- ğŸ“š Documentation complÃ¨te : `cd docs && npx mintlify dev`
- ğŸ› Logs des services : `make logs`
- ğŸ” Tests : `make test`
- ğŸ’¡ Exemples : Voir README.md et QUICKSTART.md

## ğŸ FonctionnalitÃ©s

âœ… Upload de multiples formats de documents (PDF, DOCX, TXT, MD, etc.)
âœ… Recherche sÃ©mantique vectorielle avec Qdrant
âœ… Support de plusieurs LLM (Ollama, OpenAI, Anthropic)
âœ… Architecture microservices scalable
âœ… Documentation complÃ¨te avec Mintlify
âœ… Scripts d'installation et de tests automatisÃ©s
âœ… Monitoring avec Prometheus et Grafana (optionnel)
âœ… API REST complÃ¨te avec Swagger
âœ… Gestion de collections de documents
âœ… Traitement asynchrone
âœ… Stockage distribuÃ© avec MinIO

## ğŸ’¡ Points clÃ©s de l'architecture

- **Modulaire** : Chaque service est indÃ©pendant et peut Ãªtre scalÃ© sÃ©parÃ©ment
- **Asynchrone** : Traitement non-bloquant des documents
- **Scalable** : Architecture prÃªte pour la production
- **DocumentÃ©** : Documentation complÃ¨te type Mintlify
- **TestÃ©** : Suite de tests automatisÃ©s
- **Flexible** : Support de plusieurs LLM et modÃ¨les d'embeddings

---

**PrÃªt Ã  dÃ©marrer ?**

```bash
cd /home/adminrag/openrag
make install
```

ğŸš€ Bon dÃ©veloppement avec OpenRAG !
