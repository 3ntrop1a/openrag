# Dataset Recommendations for OpenRAG Testing

## Objectif
Tester OpenRAG avec un corpus cons√©quent (500-1000+ documents) sur un domaine bien document√© pour valider les performances du syst√®me RAG.

---

## Datasets Recommand√©s

### 1. ü•á Wikipedia FR - Sciences (RECOMMAND√â)
**Source**: Dumps Wikipedia en fran√ßais  
**URL**: https://dumps.wikimedia.org/frwiki/

**Avantages**:
- ‚úÖ Corpus massif (100K+ articles scientifiques)
- ‚úÖ Contenu structur√© et v√©rifi√©
- ‚úÖ Domaine vari√©: physique, chimie, biologie, informatique
- ‚úÖ Format exploitable (XML/JSON)
- ‚úÖ Gratuit et open-source
- ‚úÖ Fran√ßais natif de qualit√©

**Inconv√©nients**:
- ‚ö†Ô∏è Tr√®s volumineux (plusieurs GB compress√©s)
- ‚ö†Ô∏è N√©cessite parsing XML
- ‚ö†Ô∏è M√©lange de qualit√© variable

**Taille**: 
- Full dump: ~20GB (compress√©)
- Articles scientifiques: ~2-5GB
- Subset 1000 articles: ~50-100MB

**Format**: XML (MediaWiki format)

---

### 2. ü•à arXiv - Computer Science Papers
**Source**: arXiv.org API  
**URL**: https://arxiv.org/help/api/

**Avantages**:
- ‚úÖ Papers scientifiques de qualit√© (peer-reviewed)
- ‚úÖ M√©tadonn√©es riches (auteurs, cat√©gories, dates)
- ‚úÖ API facile d'acc√®s
- ‚úÖ Abstracts en anglais (teste le mod√®le multilingue)
- ‚úÖ PDFs disponibles
- ‚úÖ Gratuit

**Inconv√©nients**:
- ‚ùå Principalement en anglais
- ‚ö†Ô∏è API rate-limited (3s entre requ√™tes)
- ‚ö†Ô∏è PDFs lourds

**Taille**:
- 2M+ papers disponibles
- Computer Science: ~500K papers
- Subset 1000 papers: ~1GB (PDFs) ou ~10MB (abstracts seulement)

**Format**: XML/JSON via API, PDFs

---

### 3. ü•â HAL - Publications Scientifiques FR
**Source**: HAL (Archive ouverte fran√ßaise)  
**URL**: https://hal.science/

**Avantages**:
- ‚úÖ Contenu fran√ßais +++
- ‚úÖ Recherche fran√ßaise de qualit√©
- ‚úÖ API OAI-PMH disponible
- ‚úÖ M√©tadonn√©es riches
- ‚úÖ Multidisciplinaire
- ‚úÖ PDFs open-access

**Inconv√©nients**:
- ‚ö†Ô∏è API complexe (OAI-PMH)
- ‚ö†Ô∏è Qualit√© variable (pr√©publications)
- ‚ö†Ô∏è Moins volumineux que Wikipedia

**Taille**:
- 1M+ documents
- Subset 1000 docs: ~500MB-1GB

**Format**: XML (OAI-PMH), PDFs

---

### 4. PubMed - Abstracts M√©dicaux
**Source**: PubMed Central (NIH)  
**URL**: https://pubmed.ncbi.nlm.nih.gov/

**Avantages**:
- ‚úÖ 35M+ abstracts m√©dicaux
- ‚úÖ API gratuite et simple (E-utilities)
- ‚úÖ M√©tadonn√©es structur√©es (MeSH terms)
- ‚úÖ Contenu v√©rifi√© et cit√©
- ‚úÖ Format XML propre

**Inconv√©nients**:
- ‚ùå Anglais uniquement
- ‚ö†Ô∏è Domaine tr√®s sp√©cialis√© (m√©decine)
- ‚ö†Ô∏è API rate-limited

**Taille**:
- 35M+ abstracts
- Subset 1000 abstracts: ~5MB

**Format**: XML, JSON via API

---

### 5. Project Gutenberg - Litt√©rature Classique
**Source**: Project Gutenberg  
**URL**: https://www.gutenberg.org/

**Avantages**:
- ‚úÖ 70K+ livres libres de droits
- ‚úÖ Fran√ßais disponible (~3K livres)
- ‚úÖ Texte brut facile √† parser
- ‚úÖ Contenu narratif (teste compr√©hension contextuelle)
- ‚úÖ Gratuit

**Inconv√©nients**:
- ‚ö†Ô∏è Pas scientifique (litt√©rature)
- ‚ö†Ô∏è Fran√ßais limit√© (3K livres)
- ‚ö†Ô∏è Ancien fran√ßais parfois difficile

**Taille**:
- Livres FR: ~3K
- Subset 100 livres: ~50MB

**Format**: TXT, HTML, EPUB

---

## üí° Recommandation pour Soutenance

### Option 1: Wikipedia FR Sciences (1000 articles)
**Meilleur choix pour d√©monstration √©quilibr√©e**

```bash
# T√©l√©charger subset Wikipedia Sciences (pr√©par√©)
wget https://dumps.wikimedia.org/frwiki/latest/frwiki-latest-pages-articles.xml.bz2

# Ou utiliser script de parsing (√† cr√©er)
python scripts/download_wikipedia_fr.py --category "Sciences" --limit 1000
```

**Avantages pour soutenance**:
- ‚úÖ Contenu fran√ßais ‚Üí montre gestion multilingue
- ‚úÖ Domaine scientifique ‚Üí relevant pour RAG technique
- ‚úÖ 1000 articles = ~50-100MB ‚Üí raisonnable √† processer
- ‚úÖ Diversit√© th√©matique ‚Üí teste g√©n√©ralisation
- ‚úÖ Questions vari√©es possibles (physicien c√©l√®bre, th√©orie X, etc.)

**Temps de processing estim√©**:
- 1000 articles √ó 2000 chars/chunk = ~5000-10000 chunks
- Embedding 768D: ~2-3h de processing
- Stockage Qdrant: ~500MB RAM

---

### Option 2: arXiv Computer Science (1000 abstracts)
**Bon choix pour domaine technique**

```bash
# Utiliser API arXiv
python scripts/download_arxiv.py --category "cs.AI" --limit 1000
```

**Avantages**:
- ‚úÖ Domaine informatique/AI ‚Üí pertinent pour projet RAG
- ‚úÖ Abstracts courts ‚Üí processing rapide
- ‚úÖ M√©tadonn√©es riches ‚Üí tests avanc√©s
- ‚úÖ Anglais ‚Üí teste mod√®le multilingue

**Temps de processing**:
- 1000 abstracts √ó ~2000 chars = ~2000-3000 chunks
- Embedding: ~30-60 minutes
- Stockage: ~200MB RAM

---

## Scripts de T√©l√©chargement

### Script 1: Wikipedia FR Sciences

```python
# scripts/datasets/download_wikipedia.py
import requests
import xml.etree.ElementTree as ET
from typing import List, Dict
import json

def download_wikipedia_fr_sciences(limit: int = 1000) -> List[Dict]:
    """
    Download French Wikipedia articles from Sciences category
    
    Args:
        limit: Number of articles to download
        
    Returns:
        List of articles with title, content, url
    """
    
    # Using Wikipedia API
    API_URL = "https://fr.wikipedia.org/w/api.php"
    
    articles = []
    
    params = {
        "action": "query",
        "format": "json",
        "list": "categorymembers",
        "cmtitle": "Cat√©gorie:Sciences",
        "cmlimit": limit,
        "cmnamespace": 0  # Main namespace only
    }
    
    response = requests.get(API_URL, params=params)
    members = response.json()["query"]["categorymembers"]
    
    for member in members[:limit]:
        # Get full article content
        content_params = {
            "action": "query",
            "format": "json",
            "titles": member["title"],
            "prop": "extracts",
            "explaintext": True
        }
        
        content_response = requests.get(API_URL, params=content_params)
        pages = content_response.json()["query"]["pages"]
        
        for page_id, page_data in pages.items():
            if "extract" in page_data:
                articles.append({
                    "title": page_data["title"],
                    "content": page_data["extract"],
                    "url": f"https://fr.wikipedia.org/wiki/{page_data['title'].replace(' ', '_')}",
                    "source": "wikipedia_fr"
                })
        
        if len(articles) >= limit:
            break
    
    return articles

if __name__ == "__main__":
    print("üì• Downloading Wikipedia FR Sciences articles...")
    articles = download_wikipedia_fr_sciences(limit=1000)
    
    # Save to JSON
    with open("/tmp/wikipedia_fr_sciences_1000.json", "w", encoding="utf-8") as f:
        json.dump(articles, f, ensure_ascii=False, indent=2)
    
    print(f"‚úÖ Downloaded {len(articles)} articles")
    print(f"üíæ Saved to /tmp/wikipedia_fr_sciences_1000.json")
```

---

### Script 2: arXiv Computer Science

```python
# scripts/datasets/download_arxiv.py
import arxiv
import time
from typing import List, Dict
import json

def download_arxiv_papers(category: str = "cs.AI", limit: int = 1000) -> List[Dict]:
    """
    Download arXiv papers from specific category
    
    Args:
        category: arXiv category (cs.AI, cs.CL, cs.LG, etc.)
        limit: Number of papers to download
        
    Returns:
        List of papers with title, abstract, authors, etc.
    """
    
    papers = []
    
    # Search query
    search = arxiv.Search(
        query=f"cat:{category}",
        max_results=limit,
        sort_by=arxiv.SortCriterion.SubmittedDate
    )
    
    for result in search.results():
        papers.append({
            "title": result.title,
            "content": result.summary,  # Abstract
            "authors": [author.name for author in result.authors],
            "url": result.entry_id,
            "published": result.published.isoformat(),
            "categories": result.categories,
            "source": "arxiv"
        })
        
        # Rate limiting
        time.sleep(0.1)
        
        if len(papers) % 100 == 0:
            print(f"üì• Downloaded {len(papers)}/{limit} papers...")
    
    return papers

if __name__ == "__main__":
    print("üì• Downloading arXiv Computer Science papers...")
    papers = download_arxiv_papers(category="cs.AI", limit=1000)
    
    # Save to JSON
    with open("/tmp/arxiv_cs_ai_1000.json", "w", encoding="utf-8") as f:
        json.dump(papers, f, ensure_ascii=False, indent=2)
    
    print(f"‚úÖ Downloaded {len(papers)} papers")
    print(f"üíæ Saved to /tmp/arxiv_cs_ai_1000.json")
```

---

## Import dans OpenRAG

### Script d'Import G√©n√©rique

```python
# scripts/datasets/import_to_openrag.py
import json
import requests
from pathlib import Path

def import_dataset_to_openrag(dataset_file: str, api_url: str = "http://localhost:8000"):
    """
    Import JSON dataset into OpenRAG
    
    Args:
        dataset_file: Path to JSON file with articles/papers
        api_url: OpenRAG API URL
    """
    
    with open(dataset_file, "r", encoding="utf-8") as f:
        documents = json.load(f)
    
    print(f"üìö Importing {len(documents)} documents into OpenRAG...")
    
    for i, doc in enumerate(documents):
        # Create text file
        filename = f"{doc['source']}_{i+1}_{doc['title'][:50].replace('/', '_')}.txt"
        content = f"Title: {doc['title']}\n\nContent:\n{doc['content']}\n\nSource: {doc['url']}"
        
        # Upload via API
        files = {"file": (filename, content.encode("utf-8"), "text/plain")}
        
        try:
            response = requests.post(f"{api_url}/upload", files=files)
            response.raise_for_status()
            
            if (i + 1) % 100 == 0:
                print(f"‚úÖ Imported {i + 1}/{len(documents)} documents")
        
        except Exception as e:
            print(f"‚ùå Error importing {filename}: {e}")
    
    print(f"üéâ Import complete! {len(documents)} documents processed.")

if __name__ == "__main__":
    import sys
    
    if len(sys.argv) < 2:
        print("Usage: python import_to_openrag.py <dataset.json>")
        sys.exit(1)
    
    import_dataset_to_openrag(sys.argv[1])
```

---

## Commandes Rapides

### Wikipedia FR (1000 articles)
```bash
cd /home/adminrag/openrag

# T√©l√©charger
python scripts/datasets/download_wikipedia.py

# Importer
python scripts/datasets/import_to_openrag.py /tmp/wikipedia_fr_sciences_1000.json

# Attendre processing (~2-3h)
docker-compose logs -f orchestrator

# Tester
curl -X POST http://localhost:8000/query \
  -H "Content-Type: application/json" \
  -d '{"query": "Qui a d√©couvert la relativit√© g√©n√©rale?", "use_llm": true}'
```

### arXiv CS.AI (1000 papers)
```bash
# Installer d√©pendance
pip install arxiv

# T√©l√©charger
python scripts/datasets/download_arxiv.py

# Importer
python scripts/datasets/import_to_openrag.py /tmp/arxiv_cs_ai_1000.json

# Tester
curl -X POST http://localhost:8000/query \
  -H "Content-Type: application/json" \
  -d '{"query": "What are the main approaches to attention mechanisms in neural networks?", "use_llm": true}'
```

---

## Comparaison WTE vs Dataset Scientifique

| M√©trique | WTE (actuel) | Wikipedia FR 1K | arXiv 1K |
|----------|--------------|----------------|----------|
| **Documents** | 33 | 1000 | 1000 |
| **Vecteurs** | 237 | 5000-10000 | 2000-3000 |
| **Taille** | ~35MB | ~100MB | ~10MB (abstracts) |
| **Langue** | FR | FR | EN |
| **Domaine** | Cisco/WTE | Sciences vari√©es | Computer Science |
| **Processing** | 10 min | 2-3h | 30-60 min |
| **Qualit√©** | Technique | Encyclop√©dique | Acad√©mique |
| **Coverage** | ‚ùå Faible | ‚úÖ Excellente | ‚úÖ Bonne |

---

## Prochaines √âtapes

1. ‚úÖ Choisir dataset: **Wikipedia FR Sciences (1000 articles)** RECOMMAND√â
2. ‚è≥ Cr√©er scripts de t√©l√©chargement
3. ‚è≥ T√©l√©charger dataset (~30 min)
4. ‚è≥ Importer dans OpenRAG (~1h)
5. ‚è≥ Attendre processing (~2-3h)
6. ‚è≥ Tester requ√™tes vari√©es
7. ‚è≥ Comparer avec r√©sultats WTE
8. ‚è≥ Documenter pour soutenance

**Temps total estim√©**: ~4-5h (majoritairement automatis√©)
